{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "deep-learning-experiments",
   "display_name": "Deep-Learning-Experiments"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Recognition (Dogs&Cats Excercise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The goal of this excercise is to develop and tune a neural network that would be able to recognize dogs and cats on pictures using limited data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a smaller dataset of 1000 dogs images, 1000 cats images for training; 500 dog and 500 cat images for validation; and 500 dog and 500 cat images for testing, with total of 2000 training samples, 1000 validation samples, and 1000 test samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil\n",
    "\n",
    "original_dataset_dir = \"data/full_data/train\"\n",
    "\n",
    "base_dir = \"data/partial_data\"\n",
    "os.mkdir(base_dir)\n",
    "\n",
    "# creates folders for training, validation, and testing data\n",
    "train_dir = os.path.join(base_dir, \"train\")\n",
    "os.mkdir(train_dir)\n",
    "validation_dir = os.path.join(base_dir, \"validation\")\n",
    "os.mkdir(validation_dir)\n",
    "test_dir = os.path.join(base_dir, \"test\")\n",
    "os.mkdir(test_dir)\n",
    "\n",
    "# creates individual folders for cats and for dogs for each category: training samples, validation samples, and test samples\n",
    "# this is for train samples\n",
    "train_cats_dir = os.path.join(train_dir, \"train_cats\")\n",
    "os.mkdir(train_cats_dir)\n",
    "train_dogs_dir = os.path.join(train_dir, \"train_dogs\")\n",
    "os.mkdir(train_dogs_dir)\n",
    "\n",
    "# this is for validation samples\n",
    "validation_cats_dir = os.path.join(validation_dir, \"validation_cats\")\n",
    "os.mkdir(validation_cats_dir)\n",
    "validation_dogs_dir = os.path.join(validation_dir, \"validation_dogs\")\n",
    "os.mkdir(validation_dogs_dir)\n",
    "\n",
    "# thus us for test samples\n",
    "test_cats_dir = os.path.join(test_dir, \"test_cats\")\n",
    "os.mkdir(test_cats_dir)\n",
    "test_dogs_dir = os.path.join(test_dir, \"test_dogs\")\n",
    "os.mkdir(test_dogs_dir)\n",
    "\n",
    "# copies first 1000 cats images into train_cats folder\n",
    "fnames = [\"cat.{}.jpg\".format(i) for i in range(1000)]\n",
    "for filename in fnames:\n",
    "    src = os.path.join(original_dataset_dir, filename)\n",
    "    dst = os.path.join(train_cats_dir, filename)\n",
    "    shutil.copyfile(src, dst)\n",
    "\n",
    "# copies next 500 pictures of cats into validation_cats folder\n",
    "fnames = [\"cat.{}.jpg\".format(i) for i in range(1000, 1500)]\n",
    "for filename in fnames:\n",
    "    src = os.path.join(original_dataset_dir, filename)\n",
    "    dst = os.path.join(validation_cats_dir, filename)\n",
    "    shutil.copyfile(src, dst)\n",
    "\n",
    "# copies next cats images into test_cats folder\n",
    "fnames = [\"cat.{}.jpg\".format(i) for i in range(1500, 2000)]\n",
    "for filename in fnames:\n",
    "    src = os.path.join(original_dataset_dir, filename)\n",
    "    dst = os.path.join(test_cats_dir, filename)\n",
    "    shutil.copyfile(src, dst)\n",
    "\n",
    "# copies first 1000 dogs images into train_cats folder\n",
    "fnames = [\"dog.{}.jpg\".format(i) for i in range(1000)]\n",
    "for filename in fnames:\n",
    "    src = os.path.join(original_dataset_dir, filename)\n",
    "    dst = os.path.join(train_dogs_dir, filename)\n",
    "    shutil.copyfile(src, dst)\n",
    "\n",
    "# copies next 500 pictures of dog into validation_cats folder\n",
    "fnames = [\"dog.{}.jpg\".format(i) for i in range(1000, 1500)]\n",
    "for filename in fnames:\n",
    "    src = os.path.join(original_dataset_dir, filename)\n",
    "    dst = os.path.join(validation_dogs_dir, filename)\n",
    "    shutil.copyfile(src, dst)\n",
    "\n",
    "# copies next 500 dogs images into test_cats folder\n",
    "fnames = [\"dog.{}.jpg\".format(i) for i in range(1500, 2000)]\n",
    "for filename in fnames:\n",
    "    src = os.path.join(original_dataset_dir, filename)\n",
    "    dst = os.path.join(test_dogs_dir, filename)\n",
    "    shutil.copyfile(src, dst)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here I build a model to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import models, layers, optimizers\n",
    "\n",
    "model = models.Sequential()\n",
    "\n",
    "model.add(layers.Conv2D(32, (3, 3), activation=\"relu\", input_shape=(150, 150, 3)))\n",
    "model.add(layers.MaxPool2D(2, 2))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation=\"relu\"))\n",
    "model.add(layers.MaxPool2D(2, 2))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation=\"relu\"))\n",
    "model.add(layers.MaxPool2D(2, 2))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation=\"relu\"))\n",
    "model.add(layers.MaxPool2D(2, 2))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(512, activation=\"relu\"))\n",
    "model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizers.RMSprop(learning_rate=1e-4),\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"acc\"],\n",
    ")\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here I preprocess the image data: convert it into a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Found 2000 images belonging to 2 classes.\n"
    },
    {
     "output_type": "error",
     "ename": "MemoryError",
     "evalue": "Unable to allocate 808. MiB for an array with shape (1569, 150, 150, 3) and data type float64",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-8a0cd845155c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdata_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels_batch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_generator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m     \u001b[0mx_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m     \u001b[0my_targets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_targets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mappend\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m~\\.virtualenvs\\deep-learning-experiments-pub--dWfnNaKJ\\lib\\site-packages\\numpy\\lib\\function_base.py\u001b[0m in \u001b[0;36mappend\u001b[1;34m(arr, values, axis)\u001b[0m\n\u001b[0;32m   4691\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4692\u001b[0m         \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4693\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4694\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4695\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 808. MiB for an array with shape (1569, 150, 150, 3) and data type float64"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    \"data/partial_data/train\",\n",
    "    target_size=(150, 150),\n",
    "    class_mode=\"binary\",\n",
    ")\n",
    "\n",
    "jsnfile = open(os.path.join(\"data\\\\partial_data\\\\train\", \"processed_training_data.json\"))\n",
    "\n",
    "x_train = np.zeros((1, 150, 150, 3))\n",
    "y_targets = np.zeros((1))\n",
    "\n",
    "for data_batch, labels_batch in train_generator:\n",
    "\n",
    "    x_train = np.append(x_train, data_batch, 0)\n",
    "    y_targets = np.append(y_targets, labels_batch, 0)\n",
    "\n",
    "\n",
    "json.dump((x_train, y_targets))\n"
   ]
  }
 ]
}